{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvpM/v3lBGsjKfYWTHD1mU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Z0ujsvnVXI6y","executionInfo":{"status":"ok","timestamp":1722321593042,"user_tz":-540,"elapsed":43746,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}}},"outputs":[],"source":["%%capture --no-stderr\n","%pip install langchain langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community"]},{"cell_type":"code","source":["!pip install langchainhub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52SvO5AGXT18","executionInfo":{"status":"ok","timestamp":1722321604229,"user_tz":-540,"elapsed":4106,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}},"outputId":"34149255-e15c-4aa6-e519-9402595d0aae"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchainhub\n","  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n","Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.7.4)\n","Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n","Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n","Installing collected packages: types-requests, langchainhub\n","Successfully installed langchainhub-0.1.20 types-requests-2.32.0.20240712\n"]}]},{"cell_type":"code","source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n","\n","from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model=\"gpt-4o-mini\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdKx8It7XfOk","executionInfo":{"status":"ok","timestamp":1722321675229,"user_tz":-540,"elapsed":10662,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}},"outputId":"5d0798a8-3239-461f-8f02-2133427b618c"},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}]},{"cell_type":"code","source":["from langchain import hub\n","\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","example_messages = prompt.invoke(\n","    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",").to_messages()\n","\n","example_messages"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxieTqBMX5Z4","executionInfo":{"status":"ok","timestamp":1722321707387,"user_tz":-540,"elapsed":927,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}},"outputId":"c8243248-457c-4b92-f15b-8422b7db72f8"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["1. 3개의 블로그 포스팅 본문을 Load: WebBaseLoader 활용"],"metadata":{"id":"ajzX7wcbYf6o"}},{"cell_type":"code","source":["import bs4\n","from langchain import hub\n","from langchain_chroma import Chroma\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","urls = [\n","    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n","    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n","    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n","]\n","\n","# Load, chunk and index the contents of the blog.\n","loader = WebBaseLoader(\n","    web_paths=(urls),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()\n"],"metadata":{"collapsed":true,"id":"J_5NiXEcX6Uv","executionInfo":{"status":"ok","timestamp":1722322030856,"user_tz":-540,"elapsed":1031,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["2. 불러온 본문을 Split (Chunking) : recursive text splitter 활용"],"metadata":{"id":"1IvV-Xx_YkSI"}},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n"],"metadata":{"id":"vqBAttvkYTAz","executionInfo":{"status":"ok","timestamp":1722322053636,"user_tz":-540,"elapsed":364,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["3. Chunks 를 임베딩하여 Vector store 저장: openai, chroma 사용"],"metadata":{"id":"2BEAkddgZWO6"}},{"cell_type":"code","source":["vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))"],"metadata":{"id":"VbXT8VI7ZUrR","executionInfo":{"status":"ok","timestamp":1722322203233,"user_tz":-540,"elapsed":2832,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["4. User query = ‘agent memory’ 를 받아 관련된 chunks를 retrieve"],"metadata":{"id":"aR5K0DSnZ10j"}},{"cell_type":"code","source":["retriever = vectorstore.as_retriever()\n","# prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","# def format_docs(docs):\n","#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","# rag_chain = (\n","#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","#     | prompt\n","#     | llm\n","#     | StrOutputParser()\n","# )\n","\n","request = \"agent memory\"\n","docs = retriever.invoke(request)\n","docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Zi-5GPCZ3ax","executionInfo":{"status":"ok","timestamp":1722325252038,"user_tz":-540,"elapsed":354,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}},"outputId":"2ee22f63-76e4-48bd-bbd4-e588197a2738"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)'),\n"," Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n"," Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n"," Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:')]"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","source":["5. User query와 retrieved chunk 에 대해 relevance 가 있는지를 평가하는 시스템 프롬프트 작성: retrieval 퀄리티를 LLM 이 스스로 평가하도록 하고, 관련이 있으면 {‘relevance’: ‘yes’} 관련이 없으면 {‘relevance’: ‘no’} 라고 출력하도록 함. ( JsonOutputParser() 를 활용 )"],"metadata":{"id":"gWaCpSiRcorR"}},{"cell_type":"code","source":["from langchain import hub\n","\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","example_messages = prompt.invoke(\n","    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",").to_messages()\n","\n","example_messages"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k07HQsmucoRS","executionInfo":{"status":"ok","timestamp":1722323067879,"user_tz":-540,"elapsed":530,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}},"outputId":"81974f5c-6bc7-4d4a-af49-34baa6590845"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","from langchain_core.prompts import PromptTemplate\n","\n","for doc in docs:\n","\n","  response = doc.page_content\n","\n","  joke_query = f\"Answer the 'relevance' (yes or no) about request to response. (yes or no)\\n\\n[Request]:\\n{request}\\n\\n[Response]:\\n {response}\"\n","\n","  parser = JsonOutputParser()\n","\n","  prompt = PromptTemplate(\n","      template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n","      input_variables=[\"query\"],\n","      partial_variables={\"format_instructions\": parser.get_format_instructions()},\n","  )\n","\n","  chain = prompt | llm | parser\n","  res = chain.invoke({\"query\": joke_query})\n","\n","  print(res)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O7dZtX4AdgHB","executionInfo":{"status":"ok","timestamp":1722325503398,"user_tz":-540,"elapsed":12888,"user":{"displayName":"ddingpy","userId":"01306074486401643746"}},"outputId":"a1b310bc-d186-4c77-999d-16dcaeb43789"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["{'response': {'definition': {'memory_stream': 'A long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.', 'elements': 'Each element is an observation, an event directly provided by the agent.', 'inter_agent_communication': 'Can trigger new natural language statements.'}, 'retrieval_model': {'description': 'Surfaces the context to inform the agent’s behavior, according to relevance, recency, and importance.', 'factors': {'recency': 'Recent events have higher scores.', 'importance': 'Distinguish mundane from core memories. Ask LM directly.', 'relevance': 'Based on how related it is to the current situation/query.'}}, 'reflection_mechanism': {'description': 'Synthesizes memories into higher level inferences over time and guides the agent’s future behavior.', 'note': 'Higher-level summaries of past events.'}}, 'relevance': 'yes'}\n","{'response': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\nMemory', 'relevance': 'yes'}\n","{'response': 'Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', 'relevance': 'yes'}\n","{'response': 'Fig. 13. The generative agent architecture. (Image source: Park et al. 2023) This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others). Proof-of-Concept Examples# AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing. Here is the system message used by AutoGPT, where {{...}} are user inputs: You are {{ai-name}}, {{user-provided AI bot description}}. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. GOALS:', 'relevance': 'yes'}\n"]}]}]}